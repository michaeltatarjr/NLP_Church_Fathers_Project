{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be7c013",
   "metadata": {},
   "source": [
    "## In this notebook, we will:\n",
    " * read in the files\n",
    " * compute document vectors, using Doc2Vec\n",
    " * compute the cosine similaritys\n",
    " * The end goal is to have a heatmap, showing text similarities between corpus's (this will likely include multiple heatmaps)\n",
    " * Note, start with Doc2Vec then try w/ cosine similarity, but it may be better to use soft cosine similarity.  \n",
    " * In a practial sense, this would be reading in pre-tokenized the corpora, then further tokenizing with Doc2Vec, then\n",
    " * ...building a model, then running it on whatever corpora is relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888739d",
   "metadata": {},
   "source": [
    "### General Considerations:\n",
    " 1. We might have to start small....perhaps just break off a single text, then compare it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af1541",
   "metadata": {},
   "source": [
    "#### Workflow:\n",
    "\n",
    " 1. Read docs, and tokenize w, Doc2Vec:\n",
    " 2. Train a model on a corpus to get document vectors.\n",
    " 3. Use the vectors for classification. (apparently you can also use for clustering...)\n",
    " 4. Expand to Cosine Similarity:\n",
    " 5. Calculate vectors using previous vectorization method (Doc2Vec).\n",
    " 6. Compute the cosine similarity between these vectors to determine their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19bda646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ee5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_tokenized/cf_kv_tokenized.txt', 'rb') as file:\n",
    "    cf_kv_tokenized = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "535cb08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address of Tatian to the Greeks\n",
      "Athenagoras the Athenian\n",
      "Clement of Alexandria Exhortation to the Heathen\n",
      "Clement of Alexandria The Salvation of the Rich Man\n",
      "Clement of Alexandria Trilogy\n",
      "Clement of Alexandria, The Stromata\n",
      "Dialogue of Justin, Philosopher and Martyr, with Trypho, a Jew\n",
      "Epistle of Adrian Antoninus and Marcus Aurelius\n",
      "Epistle of Clement of Rome\n",
      "False Ignatius\n",
      "Fragments of Papias\n",
      "Fragments of the Lost Work of Justin on the Resurrection\n",
      "Hippolytus of Rome Expository Treatise Against The Jews.\n",
      "Hippolytus of Rome Treatise On Christ and Antichrist\n",
      "Hippolytus of Rome\n",
      "Igantius Syriac Epistles\n",
      "Ignatius Other\n",
      "Ireanaus Against Heresies Book III\n",
      "Ireanaus Fragments from the Lost Writings\n",
      "Irenaeus Against Heresies Book I\n",
      "Irenaeus Against Heresies Book II\n",
      "Irenaeus Against Heresies Book III\n",
      "Irenaeus Against Heresies Book V\n",
      "Ireneaus Against Heresies Book IV\n",
      "Justin's Hortatory Address to the Greeks\n",
      "Origen\n",
      "Tertullian\n",
      "The Didache\n",
      "The Epistle of Barnabas\n",
      "The Epistle of Ignatius to Polycarp\n",
      "The Epistle of Ignatius to the Ephesians\n",
      "The Epistle of Ignatius to the Magnesians\n",
      "The Epistle of Ignatius to the Philadelphians\n",
      "The Epistle of Ignatius to the Romans\n",
      "The Epistle of Ignatius to the Smyrneans\n",
      "The Epistle of Ignatius to the Trallians\n",
      "The Epistle of Mathetes to Diognetus\n",
      "The Epistle of Polycarp to the Philippians\n",
      "The First Apology of Justin\n",
      "The Martyrdom of Ignatius\n",
      "The Martyrdom of Polycarp\n",
      "The Octavius of Minucius Felix\n",
      "The Pastor of Hermas\n",
      "The Second Apology of Justin for the Christians Addressed to the Roman Senate\n",
      "Theophilus of Antioch\n"
     ]
    }
   ],
   "source": [
    "for i in cf_kv_tokenized.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e75435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, what do I need to do to prep the Doc2Vec?\n",
    "#something like below.\n",
    "cf_kv_tokenized_tagged = {}\n",
    "\n",
    "for i,j in cf_kv_tokenized.items():\n",
    "    documents = doc2vec.TaggedDocument(words=j, tags=[key])\n",
    "    cf_kv_tokenized_tagged[key]=documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4719b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=cf_kv_tokenized_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c580cdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Doc2Vec(vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# Dimensionality of the document vectors\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                 window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,         \u001b[38;5;66;03m# Maximum distance between the current and predicted word within a sentence\u001b[39;00m\n\u001b[0;32m      4\u001b[0m                 min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,      \u001b[38;5;66;03m# Ignores all words with total frequency lower than this\u001b[39;00m\n\u001b[0;32m      5\u001b[0m                 workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,        \u001b[38;5;66;03m# Number of CPU cores to use for training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)        \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Build the vocabulary\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_vocab(documents)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(documents, total_examples\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcorpus_count, epochs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_vocab\u001b[39m(\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;28mself\u001b[39m, corpus_iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, corpus_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, progress_per\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m    843\u001b[0m         keep_raw_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trim_rule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    844\u001b[0m     ):\n\u001b[0;32m    845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m \n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_vocab(\n\u001b[0;32m    883\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file,\n\u001b[0;32m    884\u001b[0m         progress_per\u001b[38;5;241m=\u001b[39mprogress_per, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[1;32m-> 1054\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m   1056\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m unique tags from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m examples and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv), corpus_count, total_words,\n\u001b[0;32m   1059\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\doc2vec.py:956\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document_no, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus_iterable):\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checked_string_types:\n\u001b[1;32m--> 956\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document\u001b[38;5;241m.\u001b[39mwords, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    957\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be a list of words (usually unicode strings). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m here is instead plain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    960\u001b[0m                 \u001b[38;5;28mtype\u001b[39m(document\u001b[38;5;241m.\u001b[39mwords),\n\u001b[0;32m    961\u001b[0m             )\n\u001b[0;32m    962\u001b[0m         checked_string_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# Initialize the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50,  # Dimensionality of the document vectors\n",
    "                window=2,         # Maximum distance between the current and predicted word within a sentence\n",
    "                min_count=1,      # Ignores all words with total frequency lower than this\n",
    "                workers=4,        # Number of CPU cores to use for training\n",
    "                epochs=20)        # Number of training epochs\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(documents)\n",
    "\n",
    "# Train the model\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c09afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93002fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
